<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction摘要我们提出RNN序列模型适应文本的多标签分类问题，其中目标是一组标签，而不是序列。以前的这种RNN模型定义了序列的概率，但没有定义集合的概率;尝试获得集合概率是网络设计的后想法，包括预先指定标签顺序，或者以特定方式将序列概率与集合概率相关">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读">
<meta property="og:url" content="http://wangminrui.github.io/2018/10/01/论文阅读/index.html">
<meta property="og:site_name" content="王敏蕊个人饲养间">
<meta property="og:description" content="【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction摘要我们提出RNN序列模型适应文本的多标签分类问题，其中目标是一组标签，而不是序列。以前的这种RNN模型定义了序列的概率，但没有定义集合的概率;尝试获得集合概率是网络设计的后想法，包括预先指定标签顺序，或者以特定方式将序列概率与集合概率相关">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/AE7F9F4E5FD2404AA551CF7D0B149EBE/3387">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/F96F36680F394289A4273C040DD3D5A7/3408">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/252F89B264EB4826A66FCBE5717A3E00/3417">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/46DD98FFD1E44FA39DF52DDD0E74A3BB/3347">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/3495489F5E7849C0A694E79A96A8A6A2/3271">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BFCC069F0E0842989ED32B995DAF5B18/3314">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/6E10C024438A4877A2F9AFDECBB8B524/3324">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/CE7804265D364B5D846DA66501E3B0B3/3124">
<meta property="og:image" content="https://img-blog.csdn.net/20180423000140914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bWl5b3U4Mzg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/E2C75F3A3CF34704AA113E2BF77CAA87/3155">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/913952756B804766A5A8EB6FF2B3C3CD/3193">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/A5ECF0231C7F4BD9B63F0864DF85CEBF/3206">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/42D551E01F7B4A6DAAD85239B8711CEB/3228">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/284E07DF1ED84ABBB66A66E562618053/3214">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/F7CA278A8AFA463E8EB215BAE9F406AA/3233">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/59C6FA72414449A788CDDF981BE93ECA/3033">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/8EC145DCE51E408CA718F7877E697F3C/3038">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/FC5B742FC36647BAA3CD64BB9F8F1EFF/2949">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/137296EF92C64674BBECD970D6BA201E/2917">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BC633CF43DEC4244813F10D3159BAE9E/2896">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/01FF81C48AF14503BC71FDD99955FDAA/2805">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/5B5F356B41614CF783386337C9592942/2854">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/1E1DF43A8152406D8BEC62F68A832ADA/2717">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/562AC0A16081434B8A17C6B58356665B/2546">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/08027BAECF9B48A68ADD5041A17B1139/2569">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/82C8A9A7601B487DB0C5609B1FE2A652/2572">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/D25C2F84F3004760B7D0EC474E9138CF/2331">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/C175370FE90A4FA4B00230A625282CF6/2338">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BC9899D64A7A4F338850594109B6E4D2/1884">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/C38235F2F2304C01AB87B073A3137FEC/1933">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/093C7786ECB44DD28F185832B82430FA/2023">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/B5561986B32C45878185226CA12F5AA0/2029">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/7D4BEBD9661640DDAA8F5EE82BD026D9/2124">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/97F0C2BF467A4AF78393447B5A7371A3/2122">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/AE74E7B89C8D452CAC8483F847E43FA9/2137">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/1D0EE4835E5641EFB56C72E2CF4F1309/2402">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/5651933397F445CA9F70E145E13DFE85/2369">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BB174DE927C34E9B9A9326DB72CA2A13/2426">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/FE23E5A11C1A4E2FB5D55CD02E36D036/2432">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/3AA087D1F3F14A0DA5DDD4036BAB1CE2/2446">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/DEBC2ABB01714E93BBAB1440A44E3F18/2451">
<meta property="og:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/9473FFD1E3E44774B46E6374D8872880/2460">
<meta property="og:image" content="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png">
<meta property="og:image" content="https://qqadapt.qpic.cn/txdocpic/0/f35e76776ebb635905ed377eb0e016cd/0">
<meta property="og:image" content="https://qqadapt.qpic.cn/txdocpic/0/c00bc796ca23862369eef6f3f977c674/0">
<meta property="og:image" content="https://qqadapt.qpic.cn/txdocpic/0/5d011f541b8a951a727e977e5d1a0b95/0">
<meta property="og:updated_time" content="2019-07-30T06:26:34.469Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="论文阅读">
<meta name="twitter:description" content="【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction摘要我们提出RNN序列模型适应文本的多标签分类问题，其中目标是一组标签，而不是序列。以前的这种RNN模型定义了序列的概率，但没有定义集合的概率;尝试获得集合概率是网络设计的后想法，包括预先指定标签顺序，或者以特定方式将序列概率与集合概率相关">
<meta name="twitter:image" content="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/AE7F9F4E5FD2404AA551CF7D0B149EBE/3387">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wangminrui.github.io/2018/10/01/论文阅读/">





  <title>论文阅读 | 王敏蕊个人饲养间</title>
  
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.10'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->









</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">王敏蕊个人饲养间</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">三思而后言</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://wangminrui.github.io/2018/10/01/论文阅读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="王敏蕊">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="王敏蕊个人饲养间">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">论文阅读</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-01T09:27:08+08:00">
                2018-10-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-07-30T14:26:34+08:00">
                2019-07-30
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/备忘录/" itemprop="url" rel="index">
                    <span itemprop="name">备忘录</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/备忘录/论文/" itemprop="url" rel="index">
                    <span itemprop="name">论文</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/01/论文阅读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2018/10/01/论文阅读/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  38
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="【arxiv-2019】Adapting-RNN-Sequence-Prediction-Model-to-Multi-label-Set-Prediction"><a href="#【arxiv-2019】Adapting-RNN-Sequence-Prediction-Model-to-Multi-label-Set-Prediction" class="headerlink" title="【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction"></a>【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出RNN序列模型适应文本的多标签分类问题，其中目标是一组标签，而不是序列。以前的这种RNN模型定义了序列的概率，但没有定义集合的概率;尝试获得集合概率是网络设计的后想法，包括预先指定标签顺序，或者以特定方式将序列概率与集合概率相关联。</p>
<p>我们的公式来源于集合概率的原则概念，作为集合的相应置换序列的概率之和。我们提供了一个新的训练目标，可以最大化此设置概率，以及一个新的预测目标，可以在测试文档中找到最可能8的集合。这些新目标在理论上具有吸引力，因为它们为RNN模型提供了发现最佳标签顺序的自由，这通常是自然的（但在文档中不同）。我们开发有效的程序来解决培训和预测中涉及的计算困难。基准数据集上的实验表明，我们在这项任务中的表现优于最先进的方法。</p>
<p><a href="https://aclanthology.coli.uni-saarland.de/?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">顶会文章网址</a></p>
<p><a href="https://blog.csdn.net/liuchonge/article/details/77585222" target="_blank" rel="noopener">多标签分类paper具体的几篇介绍 从2006到2017</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/39535198" target="_blank" rel="noopener">知乎 深度学习：如何在多标签分类问题中考虑标签间的相关性？ 有两篇文章</a></p>
<h1 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h1><h2 id="【第十二周】【2018】【EMNLP】【短文本】Topic-Memory-Networks-for-Short-Text-Classification"><a href="#【第十二周】【2018】【EMNLP】【短文本】Topic-Memory-Networks-for-Short-Text-Classification" class="headerlink" title="【第十二周】【2018】【EMNLP】【短文本】Topic Memory Networks for Short Text Classification"></a>【第十二周】【2018】【EMNLP】【短文本】Topic Memory Networks for Short Text Classification</h2><ul>
<li><a href="E:\Master\paper\LiteratureManage\Topic%20Memory%20Networks%20for%20Short%20Text%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="https://arxiv.org/pdf/1809.03664.pdf" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><h3 id="future-work"><a href="#future-work" class="headerlink" title="future work"></a>future work</h3><h2 id="【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN-Learning-Hierarchical-Category-Structure-for-Multi-label-Short-Text-Categorization"><a href="#【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN-Learning-Hierarchical-Category-Structure-for-Multi-label-Short-Text-Categorization" class="headerlink" title="【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization"></a>【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization</h2><blockquote>
<p>Shimura K, Li J, Fukumoto F. HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 811-816.</p>
</blockquote>
<ul>
<li><a href="E:\Master\paper\LiteratureManage\HFT-CNN%20Learning%20Hierarchical%20Category%20Structure%20for%20Multi-label%20Short%20Text%20Categorization.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="http://www.aclweb.org/anthology/D18-1093" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h3><p>专注于短文本的多标签分类任务</p>
<p>与较高级别的类别相比，较低级别的类别是细粒度的。此外，通常情况是较低级别的训练数据量远小于较高级别的训练数据量，这会降低分类的整体性能。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>我们提出了一种方法，通过对CNN进行微调，可以有效地利用上层数据进行较低级别的分类，CNN可以学习类别的HS，并将类别的粒度纳入分类。我们根据HS调整了从上到下训练的CNN的参数，并精确调整了参数。</p>
<p>本文的主要贡献：</p>
<ol>
<li>我们提出了一种方法，可以最大化预定义类别的影响，以减轻多标签短文本中的数据稀疏性。</li>
<li>我们通过CNN实证检查了适合学习词典编纂者定义类别的HS的微调</li>
<li>结果表明我们的方法通过使用两个基准来竞争最先进的CNN方法数据集，尤其对于由具有大量标签的几个单词组成的短文本的分类是有效的。</li>
</ol>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/AE7F9F4E5FD2404AA551CF7D0B149EBE/3387" alt></p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>数据集：</p>
<ul>
<li>RCV1 集中使用了标题，所以是短文本，最长是13words</li>
<li>Amazon670K 抽取商品名称和描述中前13个words</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/F96F36680F394289A4273C040DD3D5A7/3408" alt></p>
<p>baseline:</p>
<p>XML-CNN</p>
<p>WoFT-CNN</p>
<h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/252F89B264EB4826A66FCBE5717A3E00/3417" alt></p>
<p>We have presented an approach to multi-label categorization for short text. The comparative results with XML-CNN showed that HFT-CNN is<br>competitive, especially for the cases that there exists only a small amount of training data. Future work will include: (i) incorporating lexical<br>semantics such as named entities and domainspecific senses for further improvement, (ii) extending the method to utilize label dependency<br>constraints (Bi and Kwok, 2011), and (iii) improving the accuracy of the top ranking categories to<br>deal with P@1 and NDCG@1 metrics.</p>
<h3 id="future-work-1"><a href="#future-work-1" class="headerlink" title="future work"></a>future work</h3><p>未来的工作将包括：（i）结合词汇语义，如命名实体和领域特定意义进一步改进，（ii）扩展方法以利用标签依赖性约束（Bi和Kwok，2011），以及（iii）提高准确性 处理P @ 1和NDCG @ 1指标的排名最高的类别。</p>
<h2 id="【第十二周】【arxiv】【2017】ProjectionNet-Learning-Efficient-On-Device-Deep-Networks-Using-Neural-Projections"><a href="#【第十二周】【arxiv】【2017】ProjectionNet-Learning-Efficient-On-Device-Deep-Networks-Using-Neural-Projections" class="headerlink" title="【第十二周】【arxiv】【2017】ProjectionNet Learning Efficient On-Device Deep Networks Using Neural Projections"></a>【第十二周】【arxiv】【2017】ProjectionNet Learning Efficient On-Device Deep Networks Using Neural Projections</h2><ul>
<li><a href="E:\Master\paper\LiteratureManage\ProjectionNet%20Learning%20Efficient%20On-Device%20Deep%20Networks%20Using%20Neural%20Projections.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="https://arxiv.org/pdf/1708.00630.pdf" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h3><p>深度神经网络不适合在移动设备这类有限计算能力的设备上运行</p>
<h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>我们引入了一种使用联合优化框架训练紧凑神经网络的新架构。其核心是一个新的目标，它使用两种不同类型的网络联合训练 - 全训练器神经网络（<strong>使用现有架构，如前馈NN或LSTM RNN）与更简单的“投影”网络相结合</strong>，利用随机投影来转换输入或中间表示为位。更简单的网络在位空间中编码轻量级且高效的计算操作，内存占用少。这两个网络使用反向传播进行联合训练，其中投影网络从完整的网络中学习，类似于学徒学习。一旦经过训练，较小的网络可以直接用于低内存和计算成本的推理。</p>
<p>本文的主要贡献如下：</p>
<ol>
<li>神经投影框架可以利用任何现有的深度网络，如前馈或递归神经网络，在联合优化设置中教授轻量级投影模型，该设置使用反向传播进行端到端训练。我们使用基于局部敏感散列的投影来表示轻量级网络的隐藏单元，该网络编码在推理期间非常有效计算的操作</li>
<li>该框架允许有效的分布式培训，但经过优化，可生成内存占用少的神经网络模型，可以低成本运行在设备上。模型大小可根据任务或设备容量进行参数化和配</li>
<li>我们展示了新方法在实现模型尺寸显着缩小的同时，在多种视觉和语言分类任务中提供竞争性能的有效性</li>
<li>所提出的框架进一步用于研究和表征现有深度网络的预测能力，以及紧凑地表示它们所需的神经投影比特数</li>
</ol>
<p><strong>使用了二进制局部敏感哈希</strong> LSH</p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/46DD98FFD1E44FA39DF52DDD0E74A3BB/3347" alt></p>
<h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>我们引入了一种新的神经投影方法来训练轻量级神经网络模型，以便以低计算和内存成本对设备进行有效推理。 我们展示了这种方法对模型大小和深度网络架构变化的灵活性。 在不同的视觉和语言分类任务上的实验结果表明该方法在提供显着的模型尺寸减小和有效推理的同时提供竞争性能的有效性。 我们还重新审视了深度网络的可预测性问题，并在神经位预测的背景下对此进行了研究。</p>
<h3 id="future-work-2"><a href="#future-work-2" class="headerlink" title="future work"></a>future work</h3><p>除了深度学习之外，还可以应用该框架来训练其他类型的学习场景中的轻量级模型。 例如，训练范例可以改变为半监督或无监督的设置。 可以修改训练器模型本身以结合在图形或概率图形模型而不是深度神经网络上定义的结构化损失函数。 图3示出了使用图优化损失函数学习轻量模型的端到端投影图方法，该函数可以使用大规模分布图算法甚至神经图方法进行有效训练。 投影模型训练还可以进一步扩展到涉及使用互补技术的分布式设备的场景。 我们将这些探索留作未来的工作。</p>
<h2 id="【第十二周】【EMNLP】【2018】【短文本】elf-Governing-Neural-Networks-for-On-Device-Short-Text-Classification"><a href="#【第十二周】【EMNLP】【2018】【短文本】elf-Governing-Neural-Networks-for-On-Device-Short-Text-Classification" class="headerlink" title="【第十二周】【EMNLP】【2018】【短文本】elf-Governing Neural Networks for On-Device Short Text Classification"></a>【第十二周】【EMNLP】【2018】【短文本】elf-Governing Neural Networks for On-Device Short Text Classification</h2><blockquote>
<p>Ravi S, Kozareva Z. Self-Governing Neural Networks for On-Device Short Text Classification[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 804-810.</p>
</blockquote>
<ul>
<li><a href="E:\Master\paper\LiteratureManage\Self-Governing%20Neural%20Networks%20for%20On-Device%20Short%20Text%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="http://www.aclweb.org/anthology/D18-1092" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h3><p>在微小内存占用和低计算能力的移动电话或智能手表等设备上运行这些复杂的网络</p>
<ol>
<li>微小的内存占用</li>
<li>推理延迟和</li>
<li>在云端，与高性能计算系统（如CPU，GPU和TPU）相比显着低的计算容量</li>
</ol>
<h3 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h3><p>本文的主要贡献是：</p>
<ul>
<li>用于短文本分类的设备深度学习的新型自治神经网络（SGNN）。 </li>
<li>压缩技术可有效捕获低维语义文本表示，并生成可节省存储和计算成本的紧凑模型。 </li>
<li>动态计算投影向量，无需大型预训练词嵌入或词汇修剪。</li>
<li>对话行为数据集的详尽实验评估，优于深度CNN（Lee和Dernoncourt，2016）和RNN变体（Khanpour等，2016; Ortega和Vu，2017）。</li>
</ul>
<p>模型架构：</p>
<p>SGNN：</p>
<p>该网络的自治属性源于其学习模型（例如，分类器）的能力，而无需初始化，加载或存储任何特征或词汇量权重矩阵。 从这个意义上说，我们的方法是一种<strong>真正无嵌入的方法</strong>，<strong>不像NLP中大多数广泛使用的最先进的深度学习技术，其性能取决于在大型语料库上预先训练的嵌入。 相反，我们使用投影函数将每个输入动态转换为低维表示</strong>。此外，我们将其与附加层和非线性激活相叠加，以实现投影的深度非线性组合，从而允许网络学习从输入xi到输出yi的复杂映射。</p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/3495489F5E7849C0A694E79A96A8A6A2/3271" alt></p>
<h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>对话行为的数据集</p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BFCC069F0E0842989ED32B995DAF5B18/3314" alt></p>
<h4 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h4><p>We compare our model against a majority class<br>baseline and Naive Bayes classifier (Lee and Dernoncourt, 2016). Our model significantly outperforms both baselines by 12 to 35% absolute.</p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/6E10C024438A4877A2F9AFDECBB8B524/3324" alt></p>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p>我们提出了自我管理神经网络用于设备上的短文本分类。 多个对话行为数据集的实验表明，我们的模型优于最先进的深度学习方法（Lee和Dernoncourt，2016; Khanpour等，2016; Ortega和Vu，2017）。 我们引入了一种压缩技术，<strong>可有效捕获低维语义表示，并生成可显着节省存储和计算成本的紧凑模型</strong>。 我们的方法<strong>不依赖于预先训练的嵌入，并且可以有效地计算投影向量</strong>。</p>
<h3 id="future-work-3"><a href="#future-work-3" class="headerlink" title="future work"></a>future work</h3><p>将来，我们有兴趣将此方法扩展到更自然的语言任务。 例如，我们建立了一个多语言的SGNN模型，用于客户反馈分类（Liu et al。，2017），获得73％的日本，接近最佳绩效系统（Plank，2017）。 与他们的方法不同，我们没有使用任何预处理，标记，解析，预先训练的嵌入或其他资源。</p>
<h2 id="【第十一周】【2018】【EMNLP】【胶囊网络】Investigating-Capsule-Networks-with-Dynamic-Routing-for-Text-Classification"><a href="#【第十一周】【2018】【EMNLP】【胶囊网络】Investigating-Capsule-Networks-with-Dynamic-Routing-for-Text-Classification" class="headerlink" title="【第十一周】【2018】【EMNLP】【胶囊网络】Investigating Capsule Networks with Dynamic Routing for Text Classification"></a>【第十一周】【2018】【EMNLP】【胶囊网络】Investigating Capsule Networks with Dynamic Routing for Text Classification</h2><blockquote>
<p>Yang M, Zhao W, Ye J, et al. Investigating Capsule Networks with Dynamic Routing for Text Classification[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 3110-3119.</p>
</blockquote>
<ul>
<li><a href="E:\Master\paper\LiteratureManage\Investigating%20Capsule%20Networks%20with%20Dynamic%20Routing%20for%20Text%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="https://arxiv.org/pdf/1804.00538.pdf" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为一种空间敏感模型，CNN为网格上复制特征探测器的低效性付出了代价。正如在（Sabour等，2017）中所论述的，人们必须在大小随维数增加指数增长的复制检测器和以类似指数方式增加标记训练集的体积之间进行选择。另一方面，空间不敏感的方法在判断的场景中是完全有效的，而不管任何单词或局部模式的顺序如何。但是，它们不可避免地受限于特征序列中呈现的丰富结构。因此，<strong>提高编码序列空间顺序的效率，同时保持代表能力的灵活性</strong>是一个主要问题。</p>
<blockquote>
<ul>
<li>CNN 在对空间信息进行建模时，<strong>需要对特征检测器进行复制，降低了模型的效率</strong></li>
<li>另一方面，<strong>空间不敏感的方法不可避免地受限于丰富的文本结构</strong>（比如保存单词的位置信息、语义信息、语法结构等），<strong>难以有效地进行编码且缺乏文本表达能力</strong>。</li>
</ul>
</blockquote>
<h3 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h3><p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/CE7804265D364B5D846DA66501E3B0B3/3124" alt></p>
<p><img src="https://img-blog.csdn.net/20180423000140914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bWl5b3U4Mzg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p>
<p>四层网络：</p>
<ul>
<li>ngram convolutional layer n-gram卷积层</li>
<li>primary capsule layer 初级胶囊层</li>
<li>convolutional capsule layer 卷积胶囊层</li>
<li>fully connected capsule layer 全连接胶囊层</li>
</ul>
<blockquote>
<p>连续两个卷积层采用动态路由替换池化操作</p>
</blockquote>
<p><strong>卷积层和初级胶囊层</strong></p>
<p>卷积层：提取低层特征</p>
<p>初级胶囊层：把卷积层输出的标量特征转换为矢量特征，以保留实例化参数</p>
<p><strong>路由过程</strong></p>
<p>在路由过程中，许多胶囊属于背景胶囊，它们和最终的类别胶囊没有关系，比如文本里的停用词、类别无关词等等。因此，我们提出<strong>三种策略</strong>减少背景或者噪音胶囊对网络的影响。</p>
<ol>
<li><p><strong>Orphan类别</strong>：在胶囊网络的最后一层，我们引入 Orphan 类别，它可以捕捉一些背景知识，比如停用词。在视觉任务加入 Orphan 类别效果比较有限，因为图片的背景在训练和测试集里往往是多变的。然而，在文本任务，停用词比较一致，比如谓词和代词等。</p>
</li>
<li><p><strong>Leaky-Softmax</strong>：除了在最后一层引入 Orphan 类别，中间的连续卷积层也需要引入去噪机制。对比 Orphan 类别，Leaky-Softmax 是一种轻量的去噪方法，它不需要额外的参数和计算量。</p>
</li>
</ol>
<ol>
<li><strong>路由参数修正</strong>：传统的路由参数，通常用均匀分布进行初始化，忽略了下层胶囊的概率。相反，我们把下层胶囊的概率当成路由参数的先验，改进路由过程。</li>
</ol>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/E2C75F3A3CF34704AA113E2BF77CAA87/3155" alt></p>
<p><strong>卷积胶囊层</strong></p>
<p>胶囊将乘以变换矩阵来学习孩子 - 父母关系，然后通过协议路由以在上述层中生成父胶囊</p>
<p><strong>全连接胶囊层</strong></p>
<p>下一层的胶囊被放入了一个胶囊列表中，并输入到全连接的胶囊层中，胶囊与胶囊乘以变换矩阵，然后通过协议路由产生最终的胶囊及其概率</p>
<p><strong>两种胶囊网络的架构</strong></p>
<p>Capsule-B的基本结构与Capsule-A类似，<strong>不同之处在于我们在N-gram卷积层中采用了滤波窗口（N）为3，4，5的三个并行网络</strong>。完全连接的胶囊层的最终输出被馈送到平均池中以产生最终结果。这样，Capsule-B可以学习更有意义和更全面的文本表示。 </p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/913952756B804766A5A8EB6FF2B3C3CD/3193" alt></p>
<h3 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h4><p>六个标准数据集</p>
<ul>
<li>movie reviews (MR) (Pang and Lee,2005)</li>
<li>Stanford Sentiment Treebankan extension of MR (SST-2) (Socher等，2013)</li>
<li>Subjectivity dataset (Subj) (Pang and Lee, 2004)</li>
<li>TREC questiondataset(TREC)(LiandRoth,2002)</li>
<li>customer review (CR) (Hu and Liu, 2004)</li>
<li>AG’s news corpus (Conneau等，2017). </li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/A5ECF0231C7F4BD9B63F0864DF85CEBF/3206" alt></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/42D551E01F7B4A6DAAD85239B8711CEB/3228" alt></p>
<h4 id="baseline-1"><a href="#baseline-1" class="headerlink" title="baseline"></a>baseline</h4><ul>
<li>LSTM / Bi-LSTM（Cho等，2014）</li>
<li>树型LSTM（Tree-LSTM）（Tai等，2015）</li>
<li>通过语言知识正则化的 LSTM (LR-LSTM) （Qian等，2016）</li>
<li>CNNrand/CNN-static/CNN-non-static（Kim等，2014）</li>
<li>深层卷积网络（VD-CNN）（Conneau等，2016）</li>
<li>字符级卷积网络（CL-CNN）（Zhang等，2015）</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/284E07DF1ED84ABBB66A66E562618053/3214" alt></p>
<p>CapsuleB始终比CapsuleA表现好，因为Capsule-B允许学习更有意义和更全面的文本表示</p>
<h4 id="单标签到多标签的分类迁移"><a href="#单标签到多标签的分类迁移" class="headerlink" title="单标签到多标签的分类迁移"></a>单标签到多标签的分类迁移</h4><p>单标签文字，实际上很容易收集和注释样本。然而，<strong>大规模多标签文本数据集的收集和注释负担通常非常高</strong>。由于多标签训练样本不足，神经网络（例如，CNN和LSTM）如何最好地处理多标签文本分类仍然是个问题。</p>
<p>根据（Sorower，2010），我们采用微平均精度（Precision），微平均召回（Recall）和微平均F1评分（F1）作为多标签文本分类的评估指标。这些分数中的任何一个都是首先在各个类别标签上计算出来的，然后在所有类别上进行平均，称为基于标签的度量。此外，我们还测量精确匹配率（ER），将部分正确的预测视为不正确，而只计算完全正确的样本。</p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/F7CA278A8AFA463E8EB215BAE9F406AA/3233" alt></p>
<h3 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h3><p><strong>利用胶囊的概念来改善CNN和RNN的表征局限性</strong>。（Hinton等，2011）首先介绍了“胶囊”的概念，以解决CNN和RNN的代表性局限性。具有变换矩阵的胶囊允许网络自动学习部分 - 整体关系。因此，（Sabour等，2017）提出了胶囊网络，其用矢量输出胶囊代替了CNN的标量输出特征检测器，并通过协议路由来代替最大池化。胶囊网络通过在MNIST数据上实现最新的结果显示了它的潜力。然而，<strong>与CNN中的最大池化技术不同，胶囊网络不会丢弃有关该区域内实体的准确位置的信息</strong>。对于低级胶囊，位置信息被编码到较为活跃的胶囊中。（Xi等，2017）进一步测试了胶囊网络在维数较高的CIFAR数据上的应用。（Hinton等，2018）提出了一种基于EM算法的胶囊层之间的迭代路由过程，该算法在小型NORB数据集上实现了更好的精度。迄今为止，还没有工作调查NLP任务中胶囊网络的性能。这项研究在这个主题中占据领先地位。</p>
<p>在本文中，我们调查了用动态路由进行文本分类的胶囊网络。提出了三种策略来提高动态路由过程的性能，以减轻噪声胶囊的干扰。在六个文本分类基准上的大量实验显示了胶囊网络在文本分类中的有效性。更重要的是，当通过与已有效果较好的基本方法比较，测试单标签到多标签文本分类时，胶囊网络也显示出显着的改进。</p>
<h2 id="【第十周】【2018】【NAACL】【Attention】Hierarchical-Attention-Networks-for-Document-Classification"><a href="#【第十周】【2018】【NAACL】【Attention】Hierarchical-Attention-Networks-for-Document-Classification" class="headerlink" title="【第十周】【2018】【NAACL】【Attention】Hierarchical Attention Networks for Document Classification"></a>【第十周】【2018】【NAACL】【Attention】Hierarchical Attention Networks for Document Classification</h2><ul>
<li><a href="E:\Master\paper\LiteratureManage\Hierarchical%20Attention%20Networks%20for%20Document%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="http://www.aclweb.org/anthology/N16-1174" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<blockquote>
<p>Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016: 1480-1489.</p>
</blockquote>
<p> <strong>a hierarchical structure</strong>： words form sentences,<br>sentences form a document</p>
<p>文档层次结构应该是指所有文档都是由句子组成，句子又是由词组成</p>
<p>The key difference to previous work is that our<br>system uses context to discover when a sequence of<br>tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context.</p>
<h2 id="【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse-Mode-Identification-in-Essays"><a href="#【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse-Mode-Identification-in-Essays" class="headerlink" title="【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse Mode Identification in Essays"></a>【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse Mode Identification in Essays</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>用word2vec训练好的词向量</li>
<li>用双向GRU做多标签分类</li>
<li>固定五个标签，如果该标签的概率大于0.5则判定该标签正确。</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/59C6FA72414449A788CDDF981BE93ECA/3033" alt></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li>SVM: We use bag of ngram (unigram and bigram) features to train a support vector classifier for sentence classification</li>
<li>CNN: We implement a convolutional neural<br>network (CNN) based method (Kim, 2014),<br>as it is the state-of-the-art for sentence classification.</li>
<li>GRU: We use the sentence level representation in Figure 2(a) for sentence classification.</li>
<li>GRU-GRU(GG): This method is introduced<br>in this paper in §4.1, but it doesn’t consider<br>paragraph information.</li>
<li>GRU-GRU-SEG (GG-SEG): The model considers paragraph information on the top of GG as introduced in §4.1.1</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/8EC145DCE51E408CA718F7877E697F3C/3038" alt></p>
<h2 id="【第十周】【2018】【IJCAI】JUMPER-Learning-When-to-Make-Classification-Decisions-in-Reading"><a href="#【第十周】【2018】【IJCAI】JUMPER-Learning-When-to-Make-Classification-Decisions-in-Reading" class="headerlink" title="【第十周】【2018】【IJCAI】JUMPER: Learning When to Make Classification Decisions in Reading"></a>【第十周】【2018】【IJCAI】JUMPER: Learning When to Make Classification Decisions in Reading</h2><blockquote>
<p>Liu X, Mou L, Cui H, et al. Jumper: Learning when to make classification decisions in reading[C]. International Joint Conference on Artificial Intelligence, 2018.</p>
</blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1807.02314.pdf" target="_blank" rel="noopener">原文 在线</a></li>
<li><a href="E:\Master\paper\LiteratureManage\JUMPER%20Learning%20When%20to%20Make%20Classification%20Decisions%20in%20Reading.pdf" target="_blank" rel="noopener">原文 本地</a></li>
</ul>
<h2 id="【第十周】【2018】【ACL】【词向量方向的改进】Joint-Embedding-of-Words-and-Labels-for-Text-Classification"><a href="#【第十周】【2018】【ACL】【词向量方向的改进】Joint-Embedding-of-Words-and-Labels-for-Text-Classification" class="headerlink" title="【第十周】【2018】【ACL】【词向量方向的改进】Joint Embedding of Words and Labels for Text Classification"></a>【第十周】【2018】【ACL】【词向量方向的改进】Joint Embedding of Words and Labels for Text Classification</h2><blockquote>
<p>Wang G, Li C, Wang W, et al. Joint Embedding of Words and Labels for Text Classification[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, 1: 2321-2331.</p>
</blockquote>
<ul>
<li><a href="http://cn.arxiv.org/pdf/1805.04174v1" target="_blank" rel="noopener">原文 在线</a></li>
<li><a href="E:\Master\paper\LiteratureManage\Joint%20Embedding%20of%20Words%20and%20Labels%20for%20Text%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
</ul>
<p>主要是在词向量方向做的工作<br>没看下去 过段时间看</p>
<h2 id="【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From-softmax-to-sparsemax-A-sparse-model-of-attention-and-multi-label-classification"><a href="#【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From-softmax-to-sparsemax-A-sparse-model-of-attention-and-multi-label-classification" class="headerlink" title="【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From softmax to sparsemax: A sparse model of attention and multi-label classification"></a>【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From softmax to sparsemax: A sparse model of attention and multi-label classification</h2><blockquote>
<p>Martins A, Astudillo R. From softmax to sparsemax: A sparse model of attention and multi-label classification[C]//International Conference on Machine Learning. 2016: 1614-1623.</p>
</blockquote>
<ul>
<li><a href="http://proceedings.mlr.press/v48/martins16.pdf" target="_blank" rel="noopener">原文 在线</a></li>
<li><a href="E:\Master\paper\LiteratureManage\From%20softmax%20to%20sparsemax%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
</ul>
<p>主要是提出了一种新的分类层，从softmax到sparsemax</p>
<h3 id="多标签分类实验"><a href="#多标签分类实验" class="headerlink" title="多标签分类实验"></a>多标签分类实验</h3><h4 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li>the four small-scale datasets used by<br>Koyejo et al. </li>
<li>Reuters RCV1<br>v2</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/FC5B742FC36647BAA3CD64BB9F8F1EFF/2949" alt></p>
<h4 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h4><h2 id="【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble-Application-of-Convolutional-and-Recurrent-Neural-Networks-for-Multi-label-Text-Categorization"><a href="#【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble-Application-of-Convolutional-and-Recurrent-Neural-Networks-for-Multi-label-Text-Categorization" class="headerlink" title="【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization"></a>【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization</h2><blockquote>
<p>Chen G, Ye D, Xing Z, et al. Ensemble application of convolutional and recurrent neural networks for multi-label text categorization[C]//Neural Networks (IJCNN), 2017 International Joint Conference on. IEEE, 2017: 2377-2383.</p>
</blockquote>
<ul>
<li><a href="E:\Master\paper\LiteratureManage\Ensemble%20Application%20of%20Convolutional%20and%20Recurrent%20Neural%20Networks%20for%20Multi-label%20Text%20Categorization.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7966144" target="_blank" rel="noopener">原文 在线</a></li>
</ul>
<h3 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a>主要工作</h3><ol>
<li>word2vec训练词向量</li>
<li>CNN训练从word2vec得到的词向量，从而得到一个<strong>feature vectors</strong><ul>
<li><strong>输入</strong>：the input text is<br>firstly preprocessed and tokenized into a sequence of words. Each word will look up the word embedding matrix obtained<br>from the word2vec model.The<br>original text is then a concatenation of word vectors <script type="math/tex">w_i \in R^k</script>, <script type="math/tex">S = w_{1:m} = w_1 \bigoplus w_2 \bigoplus ... \bigoplus w_m</script></li>
<li>像TextCNN一样训练，激活函数用<strong>ReLu</strong>(因为有论文证明它比sigmoid和tanh都好)，池化层用<strong>l-maxpooling</strong></li>
</ul>
</li>
<li>用RNN进行标签序列预测<ul>
<li>the <strong>standard<br>LSTM</strong> is used. An additional word embedding layer is also<br>applied for the labels.</li>
</ul>
</li>
<li>把CNN和RNN联合在一起<ul>
<li>在LSTM每一次输出前加一个softmax layer，计算最高概率的标签并输出</li>
<li>输出y 等于 上一次循环的输出，与上一次循环的隐含层、与feature vector，联合输入进行计算</li>
<li><strong>猜测</strong>：每次输出时计算softmax得出的概率都有一个阈值，如果不超过这个阈值，就认为这里没有输出标签，输出<end>，<strong>因此每次输出的标签序列是变长的</strong><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/137296EF92C64674BBECD970D6BA201E/2917" alt></end></li>
</ul>
</li>
<li>使用softmax分类器作为用于标签预测的LSTM的上层，然后将交叉熵损失从RNN向下传播回CNN以更新CNN-RNN模型的权重。</li>
</ol>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BC633CF43DEC4244813F10D3159BAE9E/2896" alt></p>
<h3 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li>Reuters-21578: 21578个文档；进行10倍交叉验证</li>
<li>RCV1-v2: 超过80万篇新闻；标签103个；进行10倍交叉验证</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/01FF81C48AF14503BC71FDD99955FDAA/2805" alt></p>
<h4 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h4><p>标签排名度量：one-error</p>
<p>？计算实例中的小部分(不在相关表中中的预测标签的top1)？</p>
<p>分类度量</p>
<ul>
<li>hamming loss 汉明损失</li>
<li>macro/micro-averaged precision 宏/微平均精确度</li>
<li>recall 召回率</li>
<li>F1 score F1积分</li>
</ul>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li>The <strong>window size</strong> is chosen up to <strong>9-gram</strong>. </li>
<li>The <strong>dimension of word vector</strong> is chosen as <strong>400</strong> while the <strong>number of filters in each window size</strong> is chosen as <strong>128</strong>. </li>
<li>The <strong>nonlinear function</strong> we choose is <strong>Relu</strong>, because [32] shows that a rectified linear unit has more benefits than sigmoid and tanh function. </li>
<li><strong>1-Max-pooling</strong> is applied in all filters so that each window size will output a <strong>128-dimension</strong> feature vector.</li>
</ul>
<h4 id="结果-4"><a href="#结果-4" class="headerlink" title="结果"></a>结果</h4><ul>
<li>Complexity of RNN versus performance: we set the CNN-RNN parameter ratio as 50% and the training epoch is limited to 50.</li>
<li>Baseline comparison<ul>
<li>baseline algorithm: <strong>Binary relevance (BR), Classifier chain (CC), MLkNN, ML-HARAM and our CNN-RNN model</strong></li>
<li>对于Reuters-21578，本文模型没有比传统模型BR、CC优秀，原因可能是Reuters-21578中每个文件的平均标签数量大约为1，因此对高阶相关性建模没有太大意义。</li>
<li>对于RCV1-V2数据集，比大多数方法优秀，原因可能是对高阶标签相关性建模有助于预测数据集中发生频率较低的次要标签。</li>
</ul>
</li>
<li>Our evaluations reveal that the power of the proposed method <strong>is affected by the size of the training dataset</strong>. If the data size is too small, the system may suffer from overfitting. However, when trained over a large-scale dataset, the proposed model can achieve the state-of-the-art performance.</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/5B5F356B41614CF783386337C9592942/2854" alt></p>
<h2 id="【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM-Sequence-Generation-Model-for-Multi-label-Classification"><a href="#【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM-Sequence-Generation-Model-for-Multi-label-Classification" class="headerlink" title="【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM: Sequence Generation Model for Multi-label Classification"></a>【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM: Sequence Generation Model for Multi-label Classification</h2><blockquote>
<p>Yang P, Sun X, Li W, et al. SGM: sequence generation model for multi-label classification[C]//Proceedings of the 27th International Conference on Computational Linguistics. 2018: 3915-3926.</p>
</blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1806.04822.pdf" target="_blank" rel="noopener">原文 在线</a></li>
<li><a href="E:\Master\paper\LiteratureManage\SGM%20Sequence%20Generation%20Model%20for%20Multi-label%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
</ul>
<h3 id="主要工作-1"><a href="#主要工作-1" class="headerlink" title="主要工作"></a>主要工作</h3><ul>
<li>将MLC(多标签分类)问题视为<strong>序列生成问题</strong>，以<strong>考虑标签之间的相关性</strong></li>
<li>提出一种新的<strong>Encoder-Decoder结构</strong>，不仅能捕获数据还能捕获标签之间的相关性，还会自动选择最有信息量的单词预测不同标签</li>
</ul>
<h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/1E1DF43A8152406D8BEC62F68A832ADA/2717" alt></p>
<h4 id="Sequence-Generation-序列生成"><a href="#Sequence-Generation-序列生成" class="headerlink" title="Sequence Generation 序列生成"></a>Sequence Generation 序列生成</h4><ul>
<li><strong>Encoder</strong><ol>
<li>将输入的one-hot representation乘以一个矩阵得到一个词向量</li>
<li>使用双向LSTM得到每个词的隐含层</li>
<li>添加注意力机制（为每个词分配权重），从而得到语境向量<script type="math/tex">c_t</script></li>
</ol>
</li>
<li><strong>Decoder（给了一堆公式计算，几乎没看懂，理解如下）</strong><ol>
<li>根据语境向量计算隐含向量<script type="math/tex">s_t</script></li>
<li>利用MS(masked softmax layer 应该是种softmax)得到对应t时刻的输出<script type="math/tex">y_t</script>，即一个标签</li>
<li><script type="math/tex; mode=display">y_t$$利用Global embedding再计算下一时刻的$$y_{t+1}</script></li>
</ol>
</li>
</ul>
<p>==问题是：最后输出标签是否固定为n个？==</p>
<h4 id="Global-Embedding-全局嵌入"><a href="#Global-Embedding-全局嵌入" class="headerlink" title="Global Embedding 全局嵌入"></a>Global Embedding 全局嵌入</h4><p> 前面预测的标签会影响后面的标签预测，称为exposure bias，<strong>如果前面预测的标签错了，会很大程度影响后面预测的标签</strong></p>
<p>通过考虑每个标签的概率，该模型能够减少由前一时间步骤中的错误预测造成的损害。</p>
<p>全局嵌入是<strong>使用变换门H的原始嵌入和加权平均嵌入的优化组合</strong>，可以自动确定每个维度中的组合因子</p>
<ul>
<li><script type="math/tex">y_{t-1}</script> 是在 <script type="math/tex">t-1</script> 时刻标签空间 <script type="math/tex">L</script> 中的概率分布</li>
<li><script type="math/tex">e</script> 是在 <script type="math/tex">y_{t-1}</script> 分布下最高概率的标签的嵌入</li>
<li><script type="math/tex">\overline{e}</script> 是在 <script type="math/tex">t</script> 时刻的具有权重的平均嵌入</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\overline&#123;e&#125; = \sum^&#123;L&#125;_&#123;i=1&#125;&#123;y_&#123;t-1&#125;^&#123;(i)&#125;e_i&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H = W_1e + W_2\overline&#123;e&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g(y_&#123;t-1&#125;) = ((1 - H)\bigodot e + H \bigodot \overline&#123;e&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集-4"><a href="#数据集-4" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li>Reuters Corpus Volume I (RCV1-V2) 80多万篇新闻，主题有103个</li>
<li>Arxiv Academic Paper Dataset (AAPD) 55,840篇论文，总共有54个科目</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/562AC0A16081434B8A17C6B58356665B/2546" alt="数据集"></p>
<h4 id="评估方法-1"><a href="#评估方法-1" class="headerlink" title="评估方法"></a>评估方法</h4><ul>
<li>汉明损失</li>
<li>micro-F1</li>
<li>micro-precision</li>
<li>micro-recall</li>
</ul>
<h3 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h3><ul>
<li>不在词汇表中的词标记为unk</li>
<li>嵌入大小512</li>
<li>LSTM层数2</li>
<li>使用最小化交叉熵损失函数来训练数据</li>
<li>使用dropout避免过度你和</li>
<li>？剪辑梯度下降到最大标准10？</li>
</ul>
<h3 id="结果-5"><a href="#结果-5" class="headerlink" title="结果"></a>结果</h3><p><strong>Baselines</strong></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/08027BAECF9B48A68ADD5041A17B1139/2569" alt="baseline"></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/82C8A9A7601B487DB0C5609B1FE2A652/2572" alt="Results"></p>
<h2 id="【第六周】【ECCV】【2016】【深度残差网络】Identity-Mappings-in-Deep-Residual-Networks"><a href="#【第六周】【ECCV】【2016】【深度残差网络】Identity-Mappings-in-Deep-Residual-Networks" class="headerlink" title="【第六周】【ECCV】【2016】【深度残差网络】Identity Mappings in Deep Residual Networks"></a>【第六周】【ECCV】【2016】【深度残差网络】Identity Mappings in Deep Residual Networks</h2><blockquote>
<p>He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.</p>
</blockquote>
<p><a href="E:\Master\paper\LiteratureManage\Identity%20Mappings%20in%20Deep%20Residual%20Networks.pdf" target="_blank" rel="noopener">原文 本地</a></p>
<p><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46493-0_38.pdf" target="_blank" rel="noopener">原文 在线</a></p>
<p><a href="https://blog.csdn.net/wspba/article/details/60750007" target="_blank" rel="noopener">译文博客</a></p>
<h3 id="中心思想"><a href="#中心思想" class="headerlink" title="中心思想"></a>中心思想</h3><p>这篇文章证明了当<strong>跳跃连接</strong>(skip connections)<strong>以及附加激活项**</strong>都使用恒等映射<strong>(identity mappings)</strong>时，前向和后向的信号能够直接的从一个block 传递到其他任意一个block。</p>
<p><strong>跳跃连接使用恒等映射</strong>指的是跳跃过程中信息不变</p>
<p><strong>附加激活项使用恒等映射</strong>指的是最后<script type="math/tex">x_{l+1}</script>不再使用激活函数非线性化<script type="math/tex">F(x) + x_l</script></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/D25C2F84F3004760B7D0EC474E9138CF/2331" alt="恒等映射"></p>
<p>==具体的分析和消融实验过程没有看==</p>
<p><strong>预激活 pre-activation：</strong><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/C175370FE90A4FA4B00230A625282CF6/2338" alt="预激活"></p>
<p><strong>把激活函数挪位置到F路径中后，看上去就像预激活</strong></p>
<h2 id="【第六周】【ACL】【2018】【zero-shot】Zero-shot-Learning-of-Classifiers-from-Natural-Language-Quantification"><a href="#【第六周】【ACL】【2018】【zero-shot】Zero-shot-Learning-of-Classifiers-from-Natural-Language-Quantification" class="headerlink" title="【第六周】【ACL】【2018】【zero-shot】Zero-shot Learning of Classifiers from Natural Language Quantification"></a>【第六周】【ACL】【2018】【zero-shot】Zero-shot Learning of Classifiers from Natural Language Quantification</h2><blockquote>
<p>Srivastava S, Labutov I, Mitchell T. Zero-shot Learning of Classifiers from Natural Language Quantification[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, 1: 306-316.</p>
</blockquote>
<p><a href="E:\Master\paper\LiteratureManage\ACL2018\P18-1.pdf" target="_blank" rel="noopener">原文 本地</a>  P306</p>
<p><strong>基本思想</strong>：</p>
<ul>
<li>在没有labeled examples的情况下学习（zero-shot)</li>
<li>利用量词的使用频率来分类，比如经常、偶尔</li>
<li>先把自然语言映射成量词的constraint</li>
<li>然后再利用constraint分类</li>
<li>……没太看懂，也没有仔细看</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BC9899D64A7A4F338850594109B6E4D2/1884" alt></p>
<h2 id="【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal-Language-Model-Fine-tuning-for-Text-Classification"><a href="#【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal-Language-Model-Fine-tuning-for-Text-Classification" class="headerlink" title="【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal Language Model Fine-tuning for Text Classification"></a>【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal Language Model Fine-tuning for Text Classification</h2><blockquote>
<p>Howard J, Ruder S. Universal language model fine-tuning for text classification[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, 1: 328-339. </p>
</blockquote>
<p><a href="E:\Master\paper\LiteratureManage\ACL2018\P18-1.pdf" target="_blank" rel="noopener">原文 本地</a>  P328</p>
<p><a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener">原文 在线</a></p>
<h3 id="主要工作-2"><a href="#主要工作-2" class="headerlink" title="主要工作"></a>主要工作</h3><ul>
<li>提出了一个模型—<strong>ULMFiT</strong>，可以在NLP领域实现类似CV的迁移学习</li>
<li>提出了<strong>discriminative fine-tuning</strong>，<strong>slanted triangular learning rates</strong>， 和 <strong>gradual unfreezing</strong> （判别性微调，倾斜三角学习率和渐进解冻）</li>
<li>在六个具有代表性的文本分类数据集上表现明显优于最新技术，大多数数据集的<strong>误差减少了18-24％</strong></li>
<li>We show that our<br>method enables extremely sample-efficient transfer learning and perform an extensive ablation<br>analysis（我们证明了我们的方法能够实现极其样本有效的转移学习并进行广泛的消融分析）</li>
<li>提供的预训练模型和代码可以广泛使用</li>
</ul>
<h3 id="提出的方法ULMFiT"><a href="#提出的方法ULMFiT" class="headerlink" title="提出的方法ULMFiT"></a>提出的方法ULMFiT</h3><h4 id="1-通用域语言模型预训练-General-domain-LM-pretraining"><a href="#1-通用域语言模型预训练-General-domain-LM-pretraining" class="headerlink" title="1. 通用域语言模型预训练(General-domain LM pretraining)"></a>1. 通用域语言模型预训练(General-domain LM pretraining)</h4><ul>
<li>使用的通用域语料库为<strong>Wikitext-103</strong>（已经预处理的28595个维基百科文章和1.03亿个词）</li>
<li>没有细写具体的预训练方法，只给了一张图</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/C38235F2F2304C01AB87B073A3137FEC/1933" alt="image"></p>
<p>a) The LM is trained on a general-domain corpus to capture<br>general features of the language in different layers</p>
<h4 id="2-目标任务语言模型微调-Target-task-LM-fine-tuning"><a href="#2-目标任务语言模型微调-Target-task-LM-fine-tuning" class="headerlink" title="2. 目标任务语言模型微调(Target task LM fine-tuning)"></a>2. 目标任务语言模型微调(Target task LM fine-tuning)</h4><p>主要工作在于：<strong>discriminative fine-tuning</strong>，<strong>slanted triangular learning rates</strong>（判别性微调，倾斜三角学习率）</p>
<ul>
<li><strong>discriminative fine-tuning</strong><ul>
<li>常规的梯度下降学习速率在每一层都是统一的，本文中将每一层的学习速率都做了区分</li>
<li><script type="math/tex; mode=display">\theta^l_t = \theta^l_{t-1} - \eta \nabla_{\theta^l}J(\theta)</script></li>
<li>根据经验，先选择最后一层的学习速率<script type="math/tex">\eta^L</script>，前一层的学习速率为<script type="math/tex">\eta^{l-1} = \frac{\eta^l}{2.6}</script></li>
</ul>
</li>
<li><strong>slanted triangular learning rates</strong><ul>
<li>想要快速收敛到一个合适的区域，使用相同的学习速率或是annealed learning rate(退火学习速率？)并不是一个好的方法</li>
<li>倾斜三角学习速率首先线性地增加学习率，然后根据以下更新时间表线性衰减它</li>
</ul>
</li>
</ul>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/093C7786ECB44DD28F185832B82430FA/2023" alt="image"></p>
<blockquote>
<p>T是训练迭代次数</p>
<p>cut_frac = cut / T，相当于 <script type="math/tex">\eta</script>从增加到减少的迭代次数占总迭代次数的比例</p>
<p>cut是当学习速率从增加到减少时的迭代次数</p>
<p>p是增加或即将减少学习速率的迭代次数除以总迭代次数</p>
<p>ratio指最小的学习速率和最大学习速率之间的差</p>
<p><script type="math/tex">\eta_t</script>是在第t次迭代时的学习速率</p>
<p>一般来说，<em>cut_frac</em>=0.1, <em>ratio</em>=32, <script type="math/tex">\eta_{max}</script>=0.01</p>
</blockquote>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/B5561986B32C45878185226CA12F5AA0/2029" alt="image"></p>
<blockquote>
<p>横轴是迭代次数</p>
</blockquote>
<h4 id="3-目标任务分类器微调-Target-task-classifier-fine-tuning"><a href="#3-目标任务分类器微调-Target-task-classifier-fine-tuning" class="headerlink" title="3. 目标任务分类器微调(Target task classifier fine-tuning)"></a>3. 目标任务分类器微调(Target task classifier fine-tuning)</h4><ul>
<li><strong>Contact pooling</strong> 联合池化<ul>
<li>如果只考虑最后一层隐含层的状态，会丢失很多有用的信息</li>
<li>因此把上一个时间节点隐含状态<script type="math/tex">h_T</script>和所有隐含隐含状态的<em>max-pooled</em>和<em>mean-pooled</em>串联起来</li>
<li><script type="math/tex">h_c = [h_T, maxpool(H),meanpool(H)]</script>，其中<script type="math/tex">H = \{h_1,...,h_T\}</script>，<script type="math/tex">[ ]</script>表示concatenation</li>
</ul>
</li>
<li><strong>Gradual unfreezing</strong> 渐进解冻<ul>
<li>从最后一层开始渐进解冻模型，而不是一次性fine-tuning所有层</li>
<li>首先解冻最后一层，并且微调所有未解冻的层；然后解冻倒数第二层，以此类推</li>
<li>==什么是unfreezing？==</li>
</ul>
</li>
<li><strong>BPTT for Text Classification(BPT3C)</strong><ul>
<li>BPTT: backpropagation through time</li>
<li>提出BPT3C(BPTT for Text Classfication)，以解决大规模文本问题</li>
<li>首先把文档划分为大小为<em>b</em> 的固定长度的批次；在每个批次的初始状态，模型初始化为前一批次的最终状态；保持追踪平均池化和最大池化的隐含状态</li>
<li>==这一段并没有看懂==</li>
</ul>
</li>
<li><strong>Bidrectional language model</strong> 双向语言模型<ul>
<li>与训练了前向传播和反向传播的LM</li>
<li>使用BPT3C分别微调了每个LM的分类器，然后平均分类器预测</li>
<li>==也没有太看懂==</li>
</ul>
</li>
</ul>
<h3 id="实验-7"><a href="#实验-7" class="headerlink" title="实验"></a>实验</h3><ul>
<li>应用的task：情感分析，问题分类，主题分类</li>
<li><strong>数据集</strong>：IMDb，TREC-6，AG</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/7D4BEBD9661640DDAA8F5EE82BD026D9/2124" alt></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/97F0C2BF467A4AF78393447B5A7371A3/2122" alt></p>
<p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/AE74E7B89C8D452CAC8483F847E43FA9/2137" alt></p>
<h4 id="别人的博客"><a href="#别人的博客" class="headerlink" title="别人的博客"></a>别人的博客</h4><p><a href="https://www.jianshu.com/p/5b680f4fb2f2" target="_blank" rel="noopener">论文分享|《Universal Language Model Fine-tuning for Text Classification》</a></p>
<h2 id="【第六周】【ACL】【2017】【DPCNN】Deep-Pyramid-Convolutional-Neural-Networks-for-Text-Categorization"><a href="#【第六周】【ACL】【2017】【DPCNN】Deep-Pyramid-Convolutional-Neural-Networks-for-Text-Categorization" class="headerlink" title="【第六周】【ACL】【2017】【DPCNN】Deep Pyramid Convolutional Neural Networks for Text Categorization"></a>【第六周】【ACL】【2017】【DPCNN】Deep Pyramid Convolutional Neural Networks for Text Categorization</h2><blockquote>
<p>Johnson R, Zhang T. Deep pyramid convolutional neural networks for text categorization[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017, 1: 562-570.</p>
</blockquote>
<p><a href="http://www.aclweb.org/anthology/P17-1052" target="_blank" rel="noopener">原文 在线</a></p>
<p><a href="E:\Master\paper\LiteratureManage\Deep%20Pyramid%20Convolutional%20Neural%20Networks%20for%20Text%20Categorization.pdf" target="_blank" rel="noopener">原文 本地</a></p>
<h3 id="DPCNN的关键点"><a href="#DPCNN的关键点" class="headerlink" title="DPCNN的关键点"></a>DPCNN的关键点</h3><ul>
<li>下采样</li>
<li>预激活的跳跃连接和恒等映射（残差网络）</li>
<li>使用无监督训练，增强了text region embedding，以提高准确率</li>
</ul>
<h3 id="主要工作-3"><a href="#主要工作-3" class="headerlink" title="主要工作"></a>主要工作</h3><h4 id="DPCNN"><a href="#DPCNN" class="headerlink" title="DPCNN"></a>DPCNN</h4><p><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/1D0EE4835E5641EFB56C72E2CF4F1309/2402" alt="Figure 1"></p>
<h5 id="下采样-Downsampling"><a href="#下采样-Downsampling" class="headerlink" title="下采样 Downsampling"></a>下采样 Downsampling</h5><ul>
<li>池化层，使用max-pooling,size 3,stride 3,两个步长，一次采样</li>
<li>不同于其他文献增加特征映射数量（增加特征映射应该是指增加卷积核）来提高准确率，本文采用<strong>固定的特征映射数</strong>，特征映射数250，卷积核size 3</li>
<li>基于以上两点形成了一个“金字塔”，我的理解是，每次池化之后，维度下降了一半，然后再次卷积、池化</li>
<li>因此，DPCNN的计算时间受一个常数限制，是单个块的计算时间的两倍（==为什么是单个块的两倍？单个块指什么？==）<br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/5651933397F445CA9F70E145E13DFE85/2369" alt="金字塔"></li>
</ul>
<h5 id="预激活的跳跃连接"><a href="#预激活的跳跃连接" class="headerlink" title="预激活的跳跃连接"></a>预激活的跳跃连接</h5><ul>
<li>参考深度残差网络的文章，在本文中，<strong>跳跃了两层卷积层</strong></li>
<li>使用深度残差网络中的预激活pre-activation</li>
</ul>
<h5 id="无需尺寸匹配"><a href="#无需尺寸匹配" class="headerlink" title="无需尺寸匹配"></a>无需尺寸匹配</h5><ul>
<li>在下采样或者使用不同大小的特征映射时，可能会导致残差无法直接相加，需要执行尺度匹配</li>
<li>本文中采用两点解决以上问题<ul>
<li>不跳跃任何下采样的层</li>
<li>在整个网络中使用固定大小的特征映射，这也节省了计算时间</li>
</ul>
</li>
</ul>
<h3 id="区域文本嵌入"><a href="#区域文本嵌入" class="headerlink" title="区域文本嵌入"></a>区域文本嵌入</h3><ul>
<li>把k个words当成一个整体，用了三种输入：sequential input, bow input, bag-of-n-gram input，其中sequential input表现非常不好就省去了</li>
<li>使用了和ShallowCNN中相同的词向量训练方法tv-embedding</li>
</ul>
<p><strong>tv-embedding</strong></p>
<ul>
<li>tv-embedding需要两个view</li>
<li>把一个text region定义为view-1，把与其相邻的text region定义为view-2</li>
<li>用view-1预测view-2来训练神经网络</li>
</ul>
<h3 id="实验-8"><a href="#实验-8" class="headerlink" title="实验"></a>实验</h3><p><strong>数据集如下</strong><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/BB174DE927C34E9B9A9326DB72CA2A13/2426" alt="数据集"></p>
<ul>
<li>数据中的所有大写字母转为小写字母</li>
<li>词汇量大小限制在30K以内（但有文章证明这足以涵盖98%的文本）</li>
</ul>
<p><strong>实验结果如下：</strong></p>
<p><em>large dataset 的结果</em><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/FE23E5A11C1A4E2FB5D55CD02E36D036/2432" alt="实验结果"><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/3AA087D1F3F14A0DA5DDD4036BAB1CE2/2446" alt="时间复杂度比较"></p>
<p><em>small dataset 的结果</em><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/DEBC2ABB01714E93BBAB1440A44E3F18/2451" alt="small dataset"><br><img src="https://note.youdao.com/yws/public/resource/4e8c44c4911b313b8d91a4ba9dde9acb/xmlnote/9473FFD1E3E44774B46E6374D8872880/2460" alt></p>
<ul>
<li>DPCNN深度一般是15，15表示两层的7个卷积块加上一层区域嵌入</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这篇讲的是深层金字塔CNN，使用了残差网络思想+下采样造就了深层金字塔状的CNN，残差网络思想用于深层，下采样形成了金字塔。</p>
<p>实验结果比列出的所有网络都优秀</p>
<p>实验结果中反复提到了ShallowCNN，说DPCNN其实是更深的ShallowCNN</p>
<h2 id="Improved-Neural-Network-based-Multi-label-Classification-with-Better-Initialization-Leveraging-Label-Co-occurrence-2016-ACL-多标签分类"><a href="#Improved-Neural-Network-based-Multi-label-Classification-with-Better-Initialization-Leveraging-Label-Co-occurrence-2016-ACL-多标签分类" class="headerlink" title="Improved Neural Network-based Multi-label Classification with Better Initialization Leveraging Label Co-occurrence 2016 ACL 多标签分类"></a>Improved Neural Network-based Multi-label Classification with Better Initialization Leveraging Label Co-occurrence 2016 ACL 多标签分类</h2><ul>
<li><a href="E:\Master\paper\LiteratureManage\Improved%20Neural%20Network-based%20Multi-label%20Classification%20with%20Better%20Initialization%20Leveraging%20Label%20Co-occurrence.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li>主要在神经网络的权重初始化上做的文章</li>
<li>首先调查已有的数据集中出现的共现标签组合，<strong>将权重矩阵的一部分变为每个标签组合的向量</strong>，其余随机初始化</li>
<li>数据集：<ul>
<li>3133个查询用于训练；其中<strong>1695</strong>个有多标签；</li>
<li>394个查询用于评估；其中<strong>158</strong>个有多标签；</li>
<li>独立的标签一共有526个；</li>
<li>实验的CNN中，词向量维度100；卷积核1000,；输出神经元526</li>
</ul>
</li>
</ul>
<p><strong>据作者说这样可以获得标签之间的关系，这样有一种聚类的效果。相似的样本会标记为相同的标签。此外他还提出了损失函数的计算方法。</strong></p>
<h2 id="Recurrent-Convolutional-Neural-Networks-for-Text-Classification-2015"><a href="#Recurrent-Convolutional-Neural-Networks-for-Text-Classification-2015" class="headerlink" title="Recurrent Convolutional Neural Networks for Text Classification 2015"></a>Recurrent Convolutional Neural Networks for Text Classification 2015</h2><ul>
<li><a href="E:\Master\paper\LiteratureManage\Text%20Classification%20Based%20on%20CNN\Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li><a href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/" target="_blank" rel="noopener">别人的博客讲解</a></li>
<li><a href="https://blog.csdn.net/rxt2012kc/article/details/73742362" target="_blank" rel="noopener">别人的博客翻译 翻得不好</a></li>
<li>卷积层：<br>前向计算词语<script type="math/tex">w_i</script>的上文<script type="math/tex">c_l(w_i) = f (W^{(l)}c_l(w_{i-1} + W^{(sl)}e(w_{i-1}))</script>；<br>后向计算词语<script type="math/tex">w_i</script>的上文<script type="math/tex">c_r(w_i) = f (W^{(r)}c_r(w_{i-1} + W^{(sr)}e(w_{i-1}))</script>；<br>输入<script type="math/tex">x_i = [c_l(w_i);e(w_i);c_r(w_i)]</script>；<br>卷积层输出<script type="math/tex">y_i^{(2)} = tanh(W^{(2)}x_i + b^{(2)})</script></li>
<li>池化层采用max-pooling，如图</li>
<li>最后用softmax输出概率<br><img src="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png" alt="image"></li>
<li>数据集<ul>
<li>20Newsgroups</li>
<li>Fudan set</li>
<li>ACL Anthology Network</li>
<li>Stanford Sentiment Treebank</li>
</ul>
</li>
<li>结果分析：<strong>CNN只能通过定长的窗口来捕捉上下文信息，但是RCNN是通过循环结构获取更长的上下文信息</strong>;时间复杂度只有O(n)</li>
</ul>
<h2 id="Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning-2016"><a href="#Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning-2016" class="headerlink" title="Recurrent Neural Network for Text Classification with Multi-Task Learning 2016"></a>Recurrent Neural Network for Text Classification with Multi-Task Learning 2016</h2><ul>
<li><a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="noopener">原文 在线</a></li>
<li><a href="E:\Master\paper\LiteratureManage\Text%20Classification%20Based%20on%20RNN\Recurrent%20Neural%20Network%20for%20Text%20Classification%20with%20Multi-Task%20Learning.pdf" target="_blank" rel="noopener">原文 本地</a></li>
<li>基于多任务的</li>
<li>研究方法<ul>
<li>提出了三种架构的基于LSTM的共享模型</li>
<li><strong>Uniform-Layer</strong> 任务之间共享LSTM层和Embedding层</li>
<li><strong>Coupled-Layer</strong> 每个任务有自己的LSTM层 但是会相互交互</li>
<li><strong>Shared-Layer</strong> 每个任务有自己的LSTM层，又加入了双向LSTM捕捉任务共享信息</li>
</ul>
</li>
<li>实验数据集<ul>
<li>SST-1</li>
<li>SST-2</li>
<li>SUBJ</li>
<li>IMDB</li>
</ul>
</li>
<li>实验过程 没有太看懂</li>
<li>实验结果误差分析<ul>
<li>在<strong>复杂句子结构</strong>和<strong>要求推理的句子</strong>上表现不好</li>
</ul>
</li>
</ul>
<h2 id="Convolutional-Neural-Networks-for-Sentence-Classiﬁcation"><a href="#Convolutional-Neural-Networks-for-Sentence-Classiﬁcation" class="headerlink" title="Convolutional Neural Networks for Sentence Classiﬁcation"></a>Convolutional Neural Networks for Sentence Classiﬁcation</h2><ul>
<li>最早开始将CNN应用于nlp领域的论文</li>
<li><a href="E:\Master\paper\LiteratureManage\Text%20Classification%20Based%20on%20CNN\Convolutional%20Neural%20Networks%20for%20Sentence%20Classification.pdf" target="_blank" rel="noopener">原文 <strong>本地</strong></a></li>
<li><a href="https://arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="noopener">原文 <strong>在线</strong></a></li>
</ul>
<p><a href="https://blog.csdn.net/qq_31456593/article/details/77659515" target="_blank" rel="noopener">上面那篇论文的中文博客</a><br>比较好理解</p>
<p><a href="https://blog.csdn.net/liuchonge/article/details/60328365" target="_blank" rel="noopener">还是上面那篇论文的中文博客</a><br>不太好理解 但是比较详细</p>
<blockquote>
<ul>
<li>输入是词向量构成的矩阵，从上到下（用的是word2vec训练过的词向量）</li>
<li><strong>卷积核比较特别</strong>，是一个<strong>h*k</strong>的矩阵，k是词向量的维度，h是一次卷积覆盖的单词数。</li>
<li>一个卷积核(Filter)就是一种<strong>特征识别器</strong></li>
<li>池化层用<strong>最大池化max-pooling</strong>，解决了可能由于句子长度不同导致输出列向量长度不同的问题</li>
<li>全连接层，为了将pooling层输出的向量转化为我们想要的预测结果，加上一个<strong>softmax层</strong>即可</li>
<li><strong>softmax层</strong>，逻辑回归模型在多分类问题上的推广<br><a href="https://blog.csdn.net/l691899397/article/details/52291909" target="_blank" rel="noopener"><strong>CSDN博客</strong> softmax层</a>  <a href="https://blog.csdn.net/u014380165/article/details/77284921" target="_blank" rel="noopener"><strong>CSDN博客</strong> softmax、softmax loss、cross entropy</a></li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(z_j) = \frac&#123;e^&#123;z_j&#125;&#125;&#123;\sum^n_&#123;i=1&#125;e^&#123;z_i&#125;&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>softmax输出的是样本属于个各类的概率</li>
<li><strong>数据集</strong> Google News; 100 billions words; 词向量维度300;</li>
<li>四种模型:</li>
<li><ul>
<li>CNN-rand: 所有词向量通过随机初始化得来，然后用CNN训练 </li>
</ul>
</li>
<li><ul>
<li>CNN-static: 词向量通过word2vec得来，然后用CNN训练；如果存在词未知对应词向量则随机初始化，并且词向量在训练过程中不会改变</li>
</ul>
</li>
<li><ul>
<li>CNN-non-static: 词向量来源同上一条，但词向量会在训练过程中进行微调</li>
</ul>
</li>
<li><ul>
<li>CNN-multichannel: 两组词向量作为两个通道同时输入</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h1><h2 id="硕博论文"><a href="#硕博论文" class="headerlink" title="硕博论文"></a>硕博论文</h2><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><h4 id="基于深度学习的知乎标题的多标签文本分类-北交专硕-2018"><a href="#基于深度学习的知乎标题的多标签文本分类-北交专硕-2018" class="headerlink" title="基于深度学习的知乎标题的多标签文本分类 北交专硕 2018"></a>基于深度学习的知乎标题的多标签文本分类 北交专硕 2018</h4><ul>
<li><a href="E:\Master\paper\LiteratureManage\Chinese\3.caj" target="_blank" rel="noopener">原文 本地</a></li>
<li><strong>CNN</strong> 和 <strong>LSTM</strong>  和  <strong>CNN-LSTM</strong></li>
<li>数据集 291433条知乎数据，百分之十测试集，百分之十验证集，百分之八十训练集</li>
<li>没什么创新 就是数据实验看上去比较真实</li>
</ul>
<p><img src="https://qqadapt.qpic.cn/txdocpic/0/f35e76776ebb635905ed377eb0e016cd/0" alt></p>
<h3 id="非神经网络"><a href="#非神经网络" class="headerlink" title="非神经网络"></a>非神经网络</h3><h4 id="基于新浪微博的短文本分类与个性化推荐-北邮硕士-2018"><a href="#基于新浪微博的短文本分类与个性化推荐-北邮硕士-2018" class="headerlink" title="基于新浪微博的短文本分类与个性化推荐 北邮硕士 2018"></a>基于新浪微博的短文本分类与个性化推荐 北邮硕士 2018</h4><ul>
<li><a href="E:\Master\paper\LiteratureManage\Chinese\1.caj" target="_blank" rel="noopener">原文 本地</a></li>
<li>LDA做特征选择</li>
<li>SVM做文本分类</li>
<li>用k折交叉验证选取LDA和SVM的参数</li>
<li>感觉很水</li>
</ul>
<h2 id="期刊会议论文"><a href="#期刊会议论文" class="headerlink" title="期刊会议论文"></a>期刊会议论文</h2><h3 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h3><h4 id="基于神经网络探究标签依赖关系的多标签分类-计算机研究与发展-2018"><a href="#基于神经网络探究标签依赖关系的多标签分类-计算机研究与发展-2018" class="headerlink" title="基于神经网络探究标签依赖关系的多标签分类 计算机研究与发展 2018"></a>基于神经网络探究标签依赖关系的多标签分类 计算机研究与发展 2018</h4><ul>
<li><a href="E:\Master\paper\LiteratureManage\Chinese\4.caj" target="_blank" rel="noopener">原文 本地</a></li>
<li>解决了两个问题：<ul>
<li><strong>不能充分探究标签依赖关系</strong></li>
<li><strong>标签缺失问题</strong></li>
</ul>
</li>
<li><p>本文的贡献</p>
<ul>
<li>在神经网络顶层加入表示语义标签类之间的相似性度量矩阵<script type="math/tex">Ω \in R^{L*L}</script>，刻画标签之间的依赖关系</li>
<li>基于标签之间的依赖关系处理标签缺失问题</li>
<li>构建３层网络结构，结构简单，能够有效避免复杂网络结构所引发的梯度不稳定性问题<br>．</li>
</ul>
</li>
<li><p>数据集：</p>
</li>
</ul>
<p><img src="https://qqadapt.qpic.cn/txdocpic/0/c00bc796ca23862369eef6f3f977c674/0" alt></p>
<h3 id="非神经网络-1"><a href="#非神经网络-1" class="headerlink" title="非神经网络"></a>非神经网络</h3><h4 id="基于标签聚类的多标签分类算法-软件学报-2014-k-means"><a href="#基于标签聚类的多标签分类算法-软件学报-2014-k-means" class="headerlink" title="基于标签聚类的多标签分类算法 软件学报 2014 k-means"></a>基于标签聚类的多标签分类算法 软件学报 2014 k-means</h4><ul>
<li><a href="E:\Master\paper\LiteratureManage\Chinese\2.caj" target="_blank" rel="noopener">原文 本地</a></li>
<li>LCMLC(基于标签聚类的多标签分类算法)</li>
<li><strong>将相关度高的标签聚合在一起形成新的标签组合</strong>，以此来发现那些重要的但不可见的标签组合</li>
<li>数据集：enron, gebase, medical, yeast, tmc2007，分别有文本、生物等领域的数据</li>
</ul>
<p><img src="https://qqadapt.qpic.cn/txdocpic/0/5d011f541b8a951a727e977e5d1a0b95/0" alt></p>
<p>==什么是重要但不可见的标签？？==</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>在线乞讨</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="王敏蕊 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="王敏蕊 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    王敏蕊
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://wangminrui.github.io/2018/10/01/论文阅读/" title="论文阅读">http://wangminrui.github.io/2018/10/01/论文阅读/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/1995/07/11/tmp/" rel="next" title="tmp">
                <i class="fa fa-chevron-left"></i> tmp
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/11/Java基础概念/" rel="prev" title="Java基础概念">
                Java基础概念 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="王敏蕊">
            
              <p class="site-author-name" itemprop="name">王敏蕊</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">53</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#【arxiv-2019】Adapting-RNN-Sequence-Prediction-Model-to-Multi-label-Set-Prediction"><span class="nav-text">【arxiv 2019】Adapting RNN Sequence Prediction Model to Multi-label Set Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-text">摘要</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#英文"><span class="nav-text">英文</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十二周】【2018】【EMNLP】【短文本】Topic-Memory-Networks-for-Short-Text-Classification"><span class="nav-text">【第十二周】【2018】【EMNLP】【短文本】Topic Memory Networks for Short Text Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验"><span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#future-work"><span class="nav-text">future work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN-Learning-Hierarchical-Category-Structure-for-Multi-label-Short-Text-Categorization"><span class="nav-text">【第十二周】【2018】【EMNLP】【多标签】【短文本】HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题-1"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法-1"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-1"><span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论-1"><span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#future-work-1"><span class="nav-text">future work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十二周】【arxiv】【2017】ProjectionNet-Learning-Efficient-On-Device-Deep-Networks-Using-Neural-Projections"><span class="nav-text">【第十二周】【arxiv】【2017】ProjectionNet Learning Efficient On-Device Deep Networks Using Neural Projections</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题-2"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法-2"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-2"><span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果"><span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#future-work-2"><span class="nav-text">future work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十二周】【EMNLP】【2018】【短文本】elf-Governing-Neural-Networks-for-On-Device-Short-Text-Classification"><span class="nav-text">【第十二周】【EMNLP】【2018】【短文本】elf-Governing Neural Networks for On-Device Short Text Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题-3"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法-3"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-3"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集"><span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#baseline"><span class="nav-text">baseline</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-1"><span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#future-work-3"><span class="nav-text">future work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十一周】【2018】【EMNLP】【胶囊网络】Investigating-Capsule-Networks-with-Dynamic-Routing-for-Text-Classification"><span class="nav-text">【第十一周】【2018】【EMNLP】【胶囊网络】Investigating Capsule Networks with Dynamic Routing for Text Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题-4"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法-4"><span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-4"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集-1"><span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#baseline-1"><span class="nav-text">baseline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#单标签到多标签的分类迁移"><span class="nav-text">单标签到多标签的分类迁移</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-2"><span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十周】【2018】【NAACL】【Attention】Hierarchical-Attention-Networks-for-Document-Classification"><span class="nav-text">【第十周】【2018】【NAACL】【Attention】Hierarchical Attention Networks for Document Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse-Mode-Identification-in-Essays"><span class="nav-text">【第十周】【2017】【ACL】【把话语模式识别当做多标签序列标记问题】Discourse Mode Identification in Essays</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-text">Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验结果"><span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十周】【2018】【IJCAI】JUMPER-Learning-When-to-Make-Classification-Decisions-in-Reading"><span class="nav-text">【第十周】【2018】【IJCAI】JUMPER: Learning When to Make Classification Decisions in Reading</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十周】【2018】【ACL】【词向量方向的改进】Joint-Embedding-of-Words-and-Labels-for-Text-Classification"><span class="nav-text">【第十周】【2018】【ACL】【词向量方向的改进】Joint Embedding of Words and Labels for Text Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From-softmax-to-sparsemax-A-sparse-model-of-attention-and-multi-label-classification"><span class="nav-text">【第十周】【2016】【ICML】【新的分类层】【只看了实验应用于多标签分类的部分】From softmax to sparsemax: A sparse model of attention and multi-label classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多标签分类实验"><span class="nav-text">多标签分类实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集-2"><span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果-3"><span class="nav-text">结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble-Application-of-Convolutional-and-Recurrent-Neural-Networks-for-Multi-label-Text-Categorization"><span class="nav-text">【第九周】【2017】【IJCNN】【多标签分类】【RCNN】nsemble Application of Convolutional and Recurrent Neural Networks for Multi-label Text Categorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#主要工作"><span class="nav-text">主要工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-5"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集-3"><span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#评估方法"><span class="nav-text">评估方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数"><span class="nav-text">参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果-4"><span class="nav-text">结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM-Sequence-Generation-Model-for-Multi-label-Classification"><span class="nav-text">【第八周】【2018】【COLING】【多标签分类】【Seq2Seq】SGM: Sequence Generation Model for Multi-label Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#主要工作-1"><span class="nav-text">主要工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#细节"><span class="nav-text">细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sequence-Generation-序列生成"><span class="nav-text">Sequence Generation 序列生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Global-Embedding-全局嵌入"><span class="nav-text">Global Embedding 全局嵌入</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-6"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集-4"><span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#评估方法-1"><span class="nav-text">评估方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#细节-1"><span class="nav-text">细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-5"><span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第六周】【ECCV】【2016】【深度残差网络】Identity-Mappings-in-Deep-Residual-Networks"><span class="nav-text">【第六周】【ECCV】【2016】【深度残差网络】Identity Mappings in Deep Residual Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#中心思想"><span class="nav-text">中心思想</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第六周】【ACL】【2018】【zero-shot】Zero-shot-Learning-of-Classifiers-from-Natural-Language-Quantification"><span class="nav-text">【第六周】【ACL】【2018】【zero-shot】Zero-shot Learning of Classifiers from Natural Language Quantification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal-Language-Model-Fine-tuning-for-Text-Classification"><span class="nav-text">【第六周】【ACL】【2018】【迁移学习】【ULMFiT】Universal Language Model Fine-tuning for Text Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#主要工作-2"><span class="nav-text">主要工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提出的方法ULMFiT"><span class="nav-text">提出的方法ULMFiT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-通用域语言模型预训练-General-domain-LM-pretraining"><span class="nav-text">1. 通用域语言模型预训练(General-domain LM pretraining)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-目标任务语言模型微调-Target-task-LM-fine-tuning"><span class="nav-text">2. 目标任务语言模型微调(Target task LM fine-tuning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-目标任务分类器微调-Target-task-classifier-fine-tuning"><span class="nav-text">3. 目标任务分类器微调(Target task classifier fine-tuning)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-7"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#别人的博客"><span class="nav-text">别人的博客</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【第六周】【ACL】【2017】【DPCNN】Deep-Pyramid-Convolutional-Neural-Networks-for-Text-Categorization"><span class="nav-text">【第六周】【ACL】【2017】【DPCNN】Deep Pyramid Convolutional Neural Networks for Text Categorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DPCNN的关键点"><span class="nav-text">DPCNN的关键点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#主要工作-3"><span class="nav-text">主要工作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DPCNN"><span class="nav-text">DPCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#下采样-Downsampling"><span class="nav-text">下采样 Downsampling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#预激活的跳跃连接"><span class="nav-text">预激活的跳跃连接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#无需尺寸匹配"><span class="nav-text">无需尺寸匹配</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#区域文本嵌入"><span class="nav-text">区域文本嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验-8"><span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improved-Neural-Network-based-Multi-label-Classification-with-Better-Initialization-Leveraging-Label-Co-occurrence-2016-ACL-多标签分类"><span class="nav-text">Improved Neural Network-based Multi-label Classification with Better Initialization Leveraging Label Co-occurrence 2016 ACL 多标签分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Convolutional-Neural-Networks-for-Text-Classification-2015"><span class="nav-text">Recurrent Convolutional Neural Networks for Text Classification 2015</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning-2016"><span class="nav-text">Recurrent Neural Network for Text Classification with Multi-Task Learning 2016</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolutional-Neural-Networks-for-Sentence-Classiﬁcation"><span class="nav-text">Convolutional Neural Networks for Sentence Classiﬁcation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#中文"><span class="nav-text">中文</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#硕博论文"><span class="nav-text">硕博论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于深度学习的知乎标题的多标签文本分类-北交专硕-2018"><span class="nav-text">基于深度学习的知乎标题的多标签文本分类 北交专硕 2018</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非神经网络"><span class="nav-text">非神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于新浪微博的短文本分类与个性化推荐-北邮硕士-2018"><span class="nav-text">基于新浪微博的短文本分类与个性化推荐 北邮硕士 2018</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#期刊会议论文"><span class="nav-text">期刊会议论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络-1"><span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于神经网络探究标签依赖关系的多标签分类-计算机研究与发展-2018"><span class="nav-text">基于神经网络探究标签依赖关系的多标签分类 计算机研究与发展 2018</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非神经网络-1"><span class="nav-text">非神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于标签聚类的多标签分类算法-软件学报-2014-k-means"><span class="nav-text">基于标签聚类的多标签分类算法 软件学报 2014 k-means</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王敏蕊</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">51.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>


 


  <script src="https://unpkg.com/mermaid@8.0.0/dist/mermaid.min.js"></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>


        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
